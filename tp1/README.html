<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="./github-pandoc.css" />
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#td1" id="toc-td1">TD1</a>
<ul>
<li><a href="#lscpu" id="toc-lscpu">lscpu</a></li>
<li><a href="#partie-1-produit-matrice-matrice"
id="toc-partie-1-produit-matrice-matrice">Partie 1: Produit
matrice-matrice</a>
<ul>
<li><a href="#effet-de-la-taille-de-la-matrice"
id="toc-effet-de-la-taille-de-la-matrice">Effet de la taille de la
matrice</a></li>
<li><a href="#permutation-des-boucles"
id="toc-permutation-des-boucles">Permutation des boucles</a></li>
<li><a href="#omp-sur-la-meilleure-boucle"
id="toc-omp-sur-la-meilleure-boucle">OMP sur la meilleure
boucle</a></li>
<li><a href="#produit-par-blocs" id="toc-produit-par-blocs">Produit par
blocs</a></li>
<li><a href="#bloc-omp" id="toc-bloc-omp">Bloc + OMP</a></li>
<li><a href="#comparaison-avec-blas-eigen-et-numpy"
id="toc-comparaison-avec-blas-eigen-et-numpy">Comparaison avec BLAS,
Eigen et numpy</a></li>
</ul></li>
<li><a href="#circulation-dun-jeton-dans-un-anneau"
id="toc-circulation-dun-jeton-dans-un-anneau">Circulation d’un jeton
dans un anneau</a></li>
</ul></li>
<li><a href="#tips" id="toc-tips">Tips</a>
<ul>
<li><a href="#partie-2" id="toc-partie-2">Partie 2</a>
<ul>
<li><a href="#circulation-dun-jeton-dans-un-anneau-1"
id="toc-circulation-dun-jeton-dans-un-anneau-1">Circulation d’un jeton
dans un anneau</a></li>
<li><a href="#calcul-très-approché-de-pi"
id="toc-calcul-très-approché-de-pi">Calcul très approché de pi</a></li>
<li><a href="#diffusion-dun-entier-dans-un-réseau-hypercube"
id="toc-diffusion-dun-entier-dans-un-réseau-hypercube">diffusion d’un
entier dans un réseau hypercube</a></li>
</ul></li>
</ul></li>
<li><a href="#questions-rencontrées"
id="toc-questions-rencontrées">Questions Rencontrées</a></li>
</ul>
</nav>
<h1 id="td1">TD1</h1>
<p><code>pandoc -s --toc README.md --css=./github-pandoc.css -o README.html</code></p>
<h2 id="lscpu">lscpu</h2>
<p><em>lscpu donne des infos utiles sur le processeur : nb core, taille
de cache :</em></p>
<pre><code>$ grep &#39;cpu cores&#39; /proc/cpuinfo |uniq
cpu cores       : 8
$ grep &#39;cache size&#39; /proc/cpuinfo |uniq
cache size      : 18432 KB
$ wmic cpu get Name, NumberOfCores, NumberOfLogicalProcessors, L2CacheSize, L3CacheSize
</code></pre>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 12%" />
<col style="width: 34%" />
<col style="width: 16%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr>
<th>L2CacheSize</th>
<th>L3CacheSize</th>
<th>Name</th>
<th>NumberOfCores</th>
<th>NumberOfLogicalProcessors</th>
</tr>
</thead>
<tbody>
<tr>
<td>9216</td>
<td>18432</td>
<td>12th Gen Intel(R) Core(TM) i7-1260P</td>
<td>12</td>
<td>16</td>
</tr>
</tbody>
</table>
<h2 id="partie-1-produit-matrice-matrice">Partie 1: Produit
matrice-matrice</h2>
<h3 id="effet-de-la-taille-de-la-matrice">Effet de la taille de la
matrice</h3>
<table>
<thead>
<tr>
<th>n</th>
<th>MFlops</th>
</tr>
</thead>
<tbody>
<tr>
<td>1024 (origine)</td>
<td>340.291</td>
</tr>
<tr>
<td>1023</td>
<td>899.395</td>
</tr>
<tr>
<td>1025</td>
<td>504.136</td>
</tr>
</tbody>
</table>
<p><em>Expliquer les résultats.</em></p>
<p>1024 est une puissance de 2 et peut provoquer des conflits de
cache;</p>
<p>Limitations SIMD: 1023 et 1025 permettent une meilleure adaptation
des blocs SIMD, notamment avec des stratégies d’optimisation comme le
prefetching et le loop unrolling.</p>
<h3 id="permutation-des-boucles">Permutation des boucles</h3>
<p><em>Expliquer comment est compilé le code (ligne de make ou de gcc) :
on aura besoin de savoir l’optim, les paramètres, etc. Par exemple
:</em></p>
<p><code>make TestProduct.exe &amp;&amp; ./TestProduct.exe 1024</code></p>
<table>
<thead>
<tr>
<th>ordre</th>
<th>time</th>
<th>MFlops</th>
<th>MFlops(n=2048)</th>
</tr>
</thead>
<tbody>
<tr>
<td>i,j,k (origine)</td>
<td>2.73764</td>
<td>782.476</td>
<td>240.485</td>
</tr>
<tr>
<td>j,i,k</td>
<td>2.451</td>
<td>876.167</td>
<td>148.043</td>
</tr>
<tr>
<td>i,k,j</td>
<td>6.279</td>
<td>341.989</td>
<td>150.247</td>
</tr>
<tr>
<td>k,i,j</td>
<td>5.9599</td>
<td>360.219</td>
<td>128.833</td>
</tr>
<tr>
<td>j,k,i</td>
<td>0.4872</td>
<td>4407.07</td>
<td>3369.76</td>
</tr>
<tr>
<td>k,j,i</td>
<td>0.5237</td>
<td>4100.02</td>
<td>2665.43</td>
</tr>
</tbody>
</table>
<p><em>Discuter les résultats.</em></p>
<p>Les ordres jki et kji sont plus rapides, et les raisons sont les
suivantes :</p>
<p>Optimisation du modèle d’accès au cache, évitant les cache misses
présents dans l’ordre ijk.</p>
<p>Les accès à B et C deviennent plus continus, réduisant ainsi le
nombre de chargements de données depuis la mémoire par le CPU.</p>
<p>Un taux de cache hit plus élevé (Cache Hit Rate) entraîne une
amélioration des performances, augmentant les MFLOPs à plus de 3000,
alors que l’ordre ijk atteint seulement environ 1000.</p>
<h3 id="omp-sur-la-meilleure-boucle">OMP sur la meilleure boucle</h3>
<p><code>make TestProduct.exe &amp;&amp; OMP_NUM_THREADS=8 ./TestProduct.exe 1024</code></p>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 12%" />
<col style="width: 21%" />
<col style="width: 21%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th>OMP_NUM</th>
<th>MFlops</th>
<th>MFlops(n=2048)</th>
<th>MFlops(n=512)</th>
<th>MFlops(n=4096)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>4206.25</td>
<td>4230.14</td>
<td>4361.62</td>
<td>2298.66</td>
</tr>
<tr>
<td>2</td>
<td>8544.66</td>
<td>5153.08</td>
<td>8284.94</td>
<td>4051.23</td>
</tr>
<tr>
<td>3</td>
<td>12757.3</td>
<td>7831.96</td>
<td>12296.7</td>
<td>6645.5</td>
</tr>
<tr>
<td>4</td>
<td>14708.3</td>
<td>9032.61</td>
<td>16777</td>
<td>9380.09</td>
</tr>
<tr>
<td>5</td>
<td>12198.4</td>
<td>9883.64</td>
<td>14747.7</td>
<td>9316.13</td>
</tr>
<tr>
<td>6</td>
<td>15573.2</td>
<td>12032.2</td>
<td>15323.6</td>
<td>5164.32</td>
</tr>
<tr>
<td>7</td>
<td>16701.5</td>
<td>13561.3</td>
<td>15755.3</td>
<td>4244.2</td>
</tr>
<tr>
<td>8</td>
<td>19555</td>
<td>14881.2</td>
<td>16729.5</td>
<td>10794.5</td>
</tr>
</tbody>
</table>
<p><em>Tracer les courbes de speedup (pour chaque valeur de n), discuter
les résultats.</em></p>
<figure>
<img src="figure/speedup_new.png" alt="Speedup" />
<figcaption aria-hidden="true">Speedup</figcaption>
</figure>
<p>Pour les matrices de petite taille (par exemple n=512 et n=1024),
l’augmentation du nombre de threads améliore initialement les
performances, mais à un nombre élevé de threads, la croissance ralentit,
voire diminue. Cela pourrait être dû à l’augmentation des <strong>coûts
de communication</strong> et <strong>de synchronisation entre les
threads</strong>. Les performances des matrices de petite taille sont
fortement affectées par la synchronisation des threads, ce qui réduit
l’efficacité à un nombre élevé de threads.</p>
<p>Pour les matrices de grande taille (par exemple n=2048 et n=4096),
l’efficacité du multithreading est plus notable, en particulier avec 3 à
4 threads, où l’accélération (Speedup) est <strong>presque
linéaire</strong>. Pour les grandes matrices (n=4096), les ruptures et
fluctuations dans la courbe de Speedup peuvent être dues à des
limitations de bande passante mémoire ou à une diminution de
l’utilisation efficace des caches.</p>
<p>En conclusion,le multithreading est plus efficace pour les grandes
matrices. Le nombre optimal de threads doit être choisi en fonction de
la taille de la matrice, car un nombre excessif de threads peut
entraîner des coûts supplémentaires et une diminution des
performances.</p>
<h3 id="produit-par-blocs">Produit par blocs</h3>
<p><code>make TestProduct.exe &amp;&amp; ./TestProduct.exe 1024</code></p>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 12%" />
<col style="width: 21%" />
<col style="width: 21%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th>szBlock</th>
<th>MFlops</th>
<th>MFlops(n=2048)</th>
<th>MFlops(n=512)</th>
<th>MFlops(n=4096)</th>
</tr>
</thead>
<tbody>
<tr>
<td>origine (=max)</td>
<td>4620.22</td>
<td>2840.5</td>
<td>5603.37</td>
<td>2369.38</td>
</tr>
<tr>
<td>32</td>
<td>4430.47</td>
<td>3207.29</td>
<td>4492.97</td>
<td>2982.27</td>
</tr>
<tr>
<td>64</td>
<td>4237.79</td>
<td>4133.57</td>
<td>4064.41</td>
<td>1294.54</td>
</tr>
<tr>
<td>128</td>
<td>4169.28</td>
<td>2791.32</td>
<td>4776.58</td>
<td>1761.58</td>
</tr>
<tr>
<td>256</td>
<td>4331.79</td>
<td>3731.22</td>
<td>4923.96</td>
<td>2307.21</td>
</tr>
<tr>
<td>512</td>
<td>4102.66</td>
<td>3718.89</td>
<td>5139.83</td>
<td>2379.74</td>
</tr>
<tr>
<td>1024</td>
<td>4647.44</td>
<td>3545.66</td>
<td>4936.51</td>
<td>2066.93</td>
</tr>
</tbody>
</table>
<p><em>Discuter les résultats.</em></p>
<figure>
<img src="figure/szblock.png" alt="szblock" />
<figcaption aria-hidden="true">szblock</figcaption>
</figure>
<p>En général, le choix de la taille des blocs influence
considérablement les performances, car il affecte la localité mémoire et
l’utilisation du cache.</p>
<p>D’après les résultats observés dans le graphique :</p>
<p>Les différentes dimensions des matrices (n) ont des impacts variés
sur les performances.</p>
<p>Pour certaines dimensions (par exemple, n=1024 et n=2048), une taille
de bloc (szBlock) plus grande peut améliorer les performances.</p>
<p>Dans d’autres cas (comme n=4096), augmenter la taille de szBlock peut
entraîner une diminution des performances.</p>
<h3 id="bloc-omp">Bloc + OMP</h3>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr>
<th>szBlock</th>
<th>OMP_NUM</th>
<th>MFlops</th>
<th>MFlops(n=2048)</th>
<th>MFlops(n=512)</th>
<th>MFlops(n=4096)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1024</td>
<td>1</td>
<td>4633.68</td>
<td>3402.95</td>
<td>5118.14</td>
<td>2591.37</td>
</tr>
<tr>
<td>1024</td>
<td>8</td>
<td>4580.94</td>
<td>11272.1</td>
<td>5033.77</td>
<td>12226.9</td>
</tr>
<tr>
<td>512</td>
<td>1</td>
<td>4717.66</td>
<td>4400.86</td>
<td>4846.87</td>
<td>2267.74</td>
</tr>
<tr>
<td>512</td>
<td>8</td>
<td>13317.1</td>
<td>17138</td>
<td>4554.21</td>
<td>14247.2</td>
</tr>
</tbody>
</table>
<p><em>Discuter les résultats.</em></p>
<figure>
<img src="figure/block+thread.png" alt="szblock_omp" />
<figcaption aria-hidden="true">szblock_omp</figcaption>
</figure>
<p>Les performances multithread sont nettement supérieures à celles à un
seul thread, grâce à <strong>l’accélération due au calcul
parallèle</strong>. Lorsque szBlock=512, les performances sont
considérablement meilleures que pour szBlock=1024, en particulier pour
des dimensions de matrice moyennes (par exemple, n=2048), où les
performances atteignent un pic. Cela montre que les <strong>petites
tailles de bloc</strong> sont plus efficaces dans l’optimisation
parallèle. Pour des dimensions de matrice plus grandes (par exemple,
n=4096), les performances diminuent en raison des contraintes de
communication entre threads et de bande passante mémoire.</p>
<p>En résumé, les petites tailles de bloc (szBlock=512) montrent les
meilleures performances en cas de calcul multithread, en particulier
pour les dimensions de matrice moyennes. Bien que les grandes tailles de
bloc (szBlock=1024) soient relativement stables en mode mono-thread,
elles ne tirent pas pleinement parti de l’optimisation parallèle.</p>
<h3 id="comparaison-avec-blas-eigen-et-numpy">Comparaison avec BLAS,
Eigen et numpy</h3>
<p><em>Comparer les performances avec un calcul similaire utilisant les
bibliothèques d’algèbre linéaire BLAS, Eigen et/ou numpy.</em></p>
<table>
<thead>
<tr>
<th>bibliothèque</th>
<th>MFlops(n=1024)</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLAS</td>
<td>150571</td>
</tr>
<tr>
<td>Eigen</td>
<td>21536.8S</td>
</tr>
</tbody>
</table>
<p>Ces bibliothèques utilisent généralement la technique de
décomposition en blocs (Blocking) pour diviser les opérations, telles
que la multiplication de matrices, en blocs afin de maximiser
l’utilisation du cache. Par rapport à une implémentation manuelle, BLAS
et Eigen choisissent une taille de bloc optimale adaptée au matériel
spécifique, ce qui augmente le taux de réussite dans le cache.</p>
<p>Eigen utilise une technologie avancée de vectorisation
(Vectorization) en exploitant les jeux d’instructions SIMD (Single
Instruction, Multiple Data) du processeur (tels que AVX, AVX-512) pour
traiter les données en parallèle. En outre, ces bibliothèques sont
optimisées pour les caractéristiques des architectures de processeurs
spécifiques (comme Intel ou AMD), notamment la taille du cache, le
pipeline et l’utilisation de la bande passante mémoire.</p>
<h2 id="circulation-dun-jeton-dans-un-anneau">Circulation d’un jeton
dans un anneau</h2>
<h1 id="tips">Tips</h1>
<pre><code>    env
    OMP_NUM_THREADS=4 ./produitMatriceMatrice.exe</code></pre>
<pre><code>    $ for i in $(seq 1 4); do elap=$(OMP_NUM_THREADS=$i ./TestProductOmp.exe|grep &quot;Temps CPU&quot;|cut -d &quot; &quot; -f 7); echo -e &quot;$i\t$elap&quot;; done &gt; timers.out
</code></pre>
<h2 id="partie-2">Partie 2</h2>
<h3 id="circulation-dun-jeton-dans-un-anneau-1">Circulation d’un jeton
dans un anneau</h3>
<p><code>mpicc test_anneau.c -o test_anneau</code>
<code>mpiexec -n 8 test_anneau</code></p>
<figure>
<img src="figure/test_anneau.png" alt="test_anneau" />
<figcaption aria-hidden="true">test_anneau</figcaption>
</figure>
<h3 id="calcul-très-approché-de-pi">Calcul très approché de pi</h3>
<h4 id="openmp">OpenMP</h4>
<p><code>g++ -fopenmp -o OpenMP_pi OpenMP_pi.cpp</code></p>
<table style="width:100%;">
<colgroup>
<col style="width: 23%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr>
<th>Nombre de Thread</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
</tr>
</thead>
<tbody>
<tr>
<td>Temp de calcul(s)</td>
<td>40.451</td>
<td>22.871</td>
<td>17.545</td>
<td>12.536</td>
<td>14.83</td>
<td>13.397</td>
<td>11.069</td>
<td>9.899</td>
</tr>
<tr>
<td>L’accélération</td>
<td>1</td>
<td>1.769</td>
<td>2.306</td>
<td>3.227</td>
<td>2.728</td>
<td>3.019</td>
<td>3.654</td>
<td>4.086</td>
</tr>
</tbody>
</table>
<h4 id="mpi">MPI</h4>
<p><code>mpicxx -o calcul_pi calcul_pi.cpp</code></p>
<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 8%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr>
<th>Nombre de Processus</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
</tr>
</thead>
<tbody>
<tr>
<td>Temp de calcul(s)</td>
<td>3.868</td>
<td>1.214</td>
<td>0.926</td>
<td>0.682</td>
<td>0.586</td>
<td>0.667</td>
<td>0.632</td>
<td>0.503</td>
</tr>
<tr>
<td>L’accélération</td>
<td>1</td>
<td>3.19</td>
<td>4.18</td>
<td>5.67</td>
<td>6.600</td>
<td>5.80</td>
<td>6.12</td>
<td>7.69</td>
</tr>
</tbody>
</table>
<h4 id="python">Python</h4>
<p><code>mpiexec  -n 4 path\python_which_include_numpy compute_pi.py</code></p>
<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 8%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr>
<th>Nombre de Processus</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
</tr>
</thead>
<tbody>
<tr>
<td>Temp de calcul(s)</td>
<td>1.3266</td>
<td>1.0436</td>
<td>0.7792</td>
<td>0.609</td>
<td>0.5279</td>
<td>0.4448</td>
<td>0.3943</td>
<td>0.3728</td>
</tr>
<tr>
<td>L’accélération</td>
<td>1</td>
<td>1.271</td>
<td>1.7025</td>
<td>2.178</td>
<td>2.507</td>
<td>2.976</td>
<td>3.367</td>
<td>3.558</td>
</tr>
</tbody>
</table>
<figure>
<img src="figure/calcul_pi.png" alt="calcul_pi" />
<figcaption aria-hidden="true">calcul_pi</figcaption>
</figure>
<h3 id="diffusion-dun-entier-dans-un-réseau-hypercube">diffusion d’un
entier dans un réseau hypercube</h3>
<p><code>mpicc -o diffusion_cube diffusion_cube.c</code></p>
<figure>
<img src="figure/diffusion.png" alt="diffusion_cube" />
<figcaption aria-hidden="true">diffusion_cube</figcaption>
</figure>
<p>La principale raison pour laquelle la sortie est dans le désordre est
que l’exécution de programmes parallèles est de nature
<strong>asynchrone</strong>, en particulier lors de l’utilisation de MPI
pour la communication distribuée, où chaque tâche (ou processus)
s’exécute indépendamment.</p>
<h1 id="questions-rencontrées">Questions Rencontrées</h1>
<p><strong>Petite Question pour info : le nombre de coeurs est différent
selons les commandes ?</strong></p>
<p><strong>ET : lscpu ne marche pas pour MSYS2 ou Windows ?</strong></p>
</body>
</html>
